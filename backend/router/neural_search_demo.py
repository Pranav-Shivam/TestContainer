# -*- coding: utf-8 -*-
"""neural_search_demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q3wXTPyCiXFdhhGOTU2Pm2zpYxPFNTEf

# Neural search demo - initial indexing

Code in this notebook shows how to prepare data for indexing in a vector search engine.

It contains the following steps:

* Downloading text data which we want to search
* Initialization of pre-trained text vectorization models (with SentenceTransformer)
* Converting text data into vectors and saving it.
"""

# We will use startup descriptions in this neural search demo
# Data source: https://startups-list.com/
# It contains name, short descrition, logo and location of startups.
!wget https://storage.googleapis.com/generall-shared-data/startups_demo.json

# We use SentenceTransformer pre-trained models to convert our text into vectors.
!pip install sentence-transformers

from sentence_transformers import SentenceTransformer
import numpy as np
import json
import pandas as pd
from tqdm.notebook import tqdm

# This code will download and create a pre-trained sentence encoder

# distilbert - is a distilated (lightweight) version of BERT model
# stsb - denotes that the model was trained for Semantic Textual Similarity
# Full list of available models could be found here https://www.sbert.net/docs/pretrained_models.html
model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens', device="cuda")

df = pd.read_json('./startups_demo.json', lines=True)

"""# New Section"""

# Here we ancode all startup descriptions
# We do encoding in batches, as this reduces overhead costs and significantly speeds up the process
vectors = []
batch_size = 64
batch = []
for row in tqdm(df.itertuples()):
  description = row.alt + ". " + row.description
  batch.append(description)
  if len(batch) >= batch_size:
    vectors.append(model.encode(batch))  # Text -> vector encoding happens here
    batch = []

if len(batch) > 0:
  vectors.append(model.encode(batch))
  batch = []

vectors = np.concatenate(vectors)

# Now we have all our descriptions converted into vectors.
# We have 40474 vectors of 768 dimentions. The output layer of the model has this dimension
vectors.shape

# You can download this saved vectors and continue with rest part of the tutorial.
np.save('vectors.npy', vectors, allow_pickle=False)

!pip install qdrant_client

from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance
import numpy as np
import json

qdrant_client = QdrantClient(
            "https://1f961a12-7402-407e-a59f-4dc1c0abd56c.us-east-1-0.aws.cloud.qdrant.io", 
            prefer_grpc=True,
            api_key="B3CLTr0wLyoQdrnXxogSS89eQtQMFzY-6RIvQOdDzRHYaL4OR0pIMQ",
        )
qdrant_client.recreate_collection(
    collection_name='startups', 
    vectors_config=VectorParams(size=768, distance=Distance.COSINE),
)

fd = open('./startups_demo.json')

# payload is now an iterator over startup data
payload = map(json.loads, fd)
print(payload)

qdrant_client.upload_collection(
    collection_name='startups',
    vectors=vectors,
    payload=payload,
    ids=None,  # Vector ids will be assigned automatically
    batch_size=256  # How many vectors will be uploaded in a single request?
)



"""## Optional part - make a test query

Let's just make sure, that our vectors are correctly converted and make sense.

For this we manually search for a closest vectors of a random sample.
"""

from sklearn.metrics.pairwise import cosine_similarity

# Take a random description as a query
sample_query = df.iloc[12345].description
print(sample_query)

query_vector = model.encode(sample_query)  # Convert query description into a vector.

scores = cosine_similarity([query_vector], vectors)[0]  # Look for the most similar vectors, manually score all vectors
top_scores_ids = np.argsort(scores)[-5:][::-1]  # Select top-5 with vectors the largest scores

# Check if result similar to the query
for top_id in top_scores_ids:
  print(df.iloc[top_id].description)
  print("-----")

!pip install geonamescache

from geonamescache import GeonamesCache

# Create a GeonamesCache object
gc = GeonamesCache()

# Get all cities from the cache
cities = gc.get_cities()

# Get a list of G-20 country codes
g20_countries = ['AR', 'AU', 'BR', 'CA', 'CN', 'DE', 'FR', 'IN', 'ID', 'IT', 'JP', 'KR', 'MX', 'RU', 'SA', 'ZA', 'TR', 'GB', 'US', 'EU']

# Filter the list of cities to only include cities from G-20 countries
g20_cities = [city['name'] for city in cities.values() if city['countrycode'] in g20_countries]

# Print the list of G-20 cities
print(g20_cities)